"""
Usage:
    spike_study.py <inputfile> <outpath> [options]

Options:
    -n <cores>       Cores to use [default: 1]
    -v <verbosity>   Verbosity [default: 10]
    -m <min_adc>     Minimum ADC count value to be considered [default: 0]

extract spike data for given file.pickle
inputfile: .pickle file generated by apply_timelapse_calibration.py
outpath: path where to put generated data.pickle
"""

from joblib import Parallel, delayed
from docopt import docopt
import numpy as np
import pickle

def find_spikes(event):
    """Find spike and return characteristics"""
    min_sample_id = 5
    max_sample_id = 36
    spikes = []
    for pixel in range(7):
        for channel in event.data.dtype.names:
            i = -1
            for adc in event.data[pixel][channel][min_sample_id:max_sample_id]:
                i += 1
                if adc >= min_adc:
                    sample_id = i + min_sample_id
                    cell_id = event.header.stop_cells[pixel][channel] + sample_id
                    spikes.append([adc, pixel, channel, event.header.event_counter, cell_id, sample_id])
    return spikes


if __name__ == '__main__':
    args = docopt(
        __doc__, version='Dragon Board spike study software v.1.0'
    )

    f = open(args['<inputfile>'], "rb")
    calib_events = []
    while 1:
        try:
            calib_events.append(pickle.load(f))
        except EOFError:
            break

    calib_events = list(event for events in calib_events for event in events)

    skip = 500
    calib_events = calib_events[skip:]

    min_adc = int(args['-m']) if args['-m'] else 0

    with Parallel(int(args['-n']), verbose=10) as pool:

        data = list(
            pool(
                delayed(find_spikes)(event) for event in calib_events
            )
        )

    data = list(spike for spike in data if spike)

    flat_data = []
    for spike in data:
        if isinstance(spike[0], list):
            for subspike in spike:
                flat_data.append(subspike)
        else:
            flat_data.append(subspike)
    data = flat_data

    file = (args['<outpath>'] + 'spikes_adc{}_536.pickle'.format(min_adc))
    max_data_size = 10000

    for i in range(int(len(data) / max_data_size)):
        try:
            f = open(file, "rb")
            calib_data = []
            while 1:
                try:
                    calib_data.append(pickle.load(f))
                except EOFError:
                    break
        except:
            pass
        f = open(file, "wb")
        try:
            for data_item in calib_data:
                pickle.dump(data_item, f)
        except:
            pass
        pickle.dump(data[i * max_data_size:(i + 1) * max_data_size - 1], f)
        f.close()
